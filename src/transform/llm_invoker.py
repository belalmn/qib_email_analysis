import asyncio
import logging
from typing import Dict, List, Union

import pandas as pd
import torch
import transformers
from langchain_community.llms.ollama import Ollama
from langchain_huggingface import HuggingFacePipeline
from tqdm.asyncio import tqdm as atqdm
from tqdm import tqdm
from transformers import AutoTokenizer

tqdm.pandas()

class LLMInvoker:
    """
    A class for invoking large language models (LLMs) in the pipeline.
    
    Attributes:
        model_name (str): The name of the model to use.
        use_ollama (bool): A flag indicating whether to use the Ollama model.
        llm (Union[Ollama, HuggingFacePipeline]): The LLM used for inference.
    """
    
    def __init__(self, model_name: str = "microsoft/Phi-3-mini-4k-instruct", use_ollama: bool = False):
        """
        Initializes the LLMInvoker with the specified model.

        Args:
            model_name (str): The model name to load from Hugging Face or Ollama.
            use_ollama (bool): Flag to determine if the Ollama model should be used. Defaults to False.
        """
        self.use_ollama = use_ollama
        self.model_name = model_name
        self.llm: Union[Ollama, HuggingFacePipeline]

        if use_ollama:
            logging.info(f"Using Ollama model: {model_name}")
            self.llm = Ollama(model=model_name)
        else:
            logging.info(f"Using HuggingFace model: {model_name}")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            pipe = transformers.pipeline(
                task="text-generation",
                model=model_name,
                tokenizer=tokenizer,
                device="cuda" if torch.cuda.is_available() else "cpu",
                max_length=10000,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
            logging.info(f"Pipeline created for model {model_name}")
            self.llm = HuggingFacePipeline(pipeline=pipe)
            logging.info(f"LangChain LLM created for model {model_name}")

    def invoke_llm(self, prompt: str) -> str:
        """
        Invokes the LLM with a given prompt.

        Args:
            prompt (str): The prompt to be passed to the LLM.

        Returns:
            str: The response generated by the LLM.
        """
        return self.llm.invoke(prompt)

    def invoke_llms_df(self, df: pd.DataFrame, prompt_column_name: str) -> pd.DataFrame:
        """
        Applies the LLM invocation over a dataframe column.

        Args:
            df (pd.DataFrame): The dataframe containing the prompt column.
            prompt_column_name (str): The column name with the prompts.

        Returns:
            pd.DataFrame: The dataframe with the LLM responses.
        """
        df["llm_response"] = df[prompt_column_name].progress_apply(self.invoke_llm)
        return df

    async def ainvoke_llm(self, prompt: str) -> str:
        """
        Asynchronously invokes the LLM with a given prompt.

        Args:
            prompt (str): The prompt to be passed to the LLM.

        Returns:
            str: The response generated by the LLM.
        """
        return await self.llm.ainvoke(prompt)

    async def ainvoke_llms_df(self, df: pd.DataFrame, prompt_column_name: str) -> pd.DataFrame:
        """
        Asynchronously applies the LLM invocation over a dataframe column.

        Args:
            df (pd.DataFrame): The dataframe containing the prompt column.
            prompt_column_name (str): The column name with the prompts.

        Returns:
            pd.DataFrame: The dataframe with the LLM responses.
        """
        tasks = [self.ainvoke_llm(row[prompt_column_name]) for _, row in df.iterrows()]
        results = await atqdm.gather(*tasks)
        df["llm_response"] = results
        return df["llm_response"]
