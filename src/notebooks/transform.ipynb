{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline: Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belalm-linux/miniconda3/envs/qib/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/belalm-linux/miniconda3/envs/qib/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/belalm-linux/miniconda3/envs/qib/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:root:Using Ollama model: Z:\\AI_Models\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\c1358f8a35e6d2af81890deffbbfa575b978c62f\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../..')\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['XDG_CACHE_HOME']='Z:\\AI_Models'\n",
    "model_path = 'Z:\\AI_Models\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\c1358f8a35e6d2af81890deffbbfa575b978c62f'\n",
    "\n",
    "from src.config.config import Config\n",
    "from src.database.chroma_manager import ChromaManager\n",
    "from src.database.database import Database\n",
    "from src.load.data_loader import DataLoader\n",
    "from src.transform.email_summary import summarize_messages\n",
    "from src.transform.llm_invoker import LLMInvoker\n",
    "from src.transform.message_classification import classify_categories\n",
    "from src.transform.product_classification import classify_products\n",
    "from src.transform.ner import extract_entities_from_messages\n",
    "from src.transform.spam_classification import (\n",
    "    classify_spam_messages_with_llm,\n",
    "    zero_shot_classify_spam_messages,\n",
    ")\n",
    "from src.transform.topic_modelling import TopicModellor\n",
    "from src.utils.checkpoint import DataFrameCheckpointer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "config = Config.from_json(\"../../config.json\")\n",
    "llm_invoker = LLMInvoker(model_name=model_path, use_ollama=config.use_ollama)\n",
    "database = Database.from_credentials(username=config.db_user, password=config.db_password, host=config.db_host, database=config.db_name)\n",
    "loader = DataLoader(database)\n",
    "                         \n",
    "DATA_DIR = '../../data'\n",
    "PST_DIR = config.pst_directory\n",
    "DATE = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "checkpointer = DataFrameCheckpointer(DATA_DIR + '/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All tables have been dropped from the database.\n",
      "INFO:root:All tables have been dropped from the database.\n",
      "INFO:root:All tables have been created in the database.\n"
     ]
    }
   ],
   "source": [
    "loader.clear_all_data()\n",
    "loader.create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3505: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(f\"{DATA_DIR}/interim/sample_preprocessed_messages_2024-08-21.csv\") # Sample dataset: 500 emails\n",
    "df = pd.read_csv(f\"{DATA_DIR}/interim/preprocessed_messages_2024-09-01.csv\") # Full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Quarterly, Monthly, and Weekly Sets of Messages from DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most feature engineering tasks don't need to be run on all emails. The following feature engineering tasks are intended for customer oriented emails. We can safely disregard internal emails and outgoing emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = df.loc[(df[\"is_internal\"] == False) & (df[\"from_address\"] != \"info@qib.com.qa\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = df.dropna(subset=['message_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64933"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(message_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = message_df.drop_duplicates(subset=['message_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df[\"topic_id\"]=-1\n",
    "message_df[\"is_spam\"]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df[\"is_internal\"]=message_df[\"sender_domain\"].apply(\n",
    "        lambda x: \"qib\" in x #any(\"qib\" in domain for domain in str(x).split(\", \"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = message_df.where(pd.notnull(message_df), None)\n",
    "message_df=message_df.replace({np.nan:None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df.submit_time=pd.to_datetime(message_df.submit_time).dt.tz_localize(None)\n",
    "message_df.delivery_time=pd.to_datetime(message_df.delivery_time).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = message_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = old_df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further filter by removing spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam_df = classify_spam_messages_with_llm(message_df, llm_invoker)\n",
    "spam_df = zero_shot_classify_spam_messages(message_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointer.save(\"spam_classification\", spam_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = message_df.merge(spam_df, on=\"message_id\")\n",
    "message_df = message_df.loc[message_df[\"is_spam\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df[\"is_spam\"]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer.save(\"spam_classified_messages\", message_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = checkpointer.pull(\"spam_classified_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Sentence Transformer and ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = ChromaManager(\"message_embeddings\", model_name=config.embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get or Create Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "message_df = chroma.populate_embeddings(message_df[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer.save(\"message_embeddings\", message_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_df = checkpointer.pull(\"message_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modellor = TopicModellor(message_df, llm_invoker)\n",
    "topic_df = topic_modellor.topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_describe = topic_df[topic_df[\"topic_id\"] != -1].groupby(\"topic_id\").filter(lambda x: len(x) >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_descriptions = topic_modellor.get_topic_descriptions(topics_to_describe, llm_invoker)[[\"topic_id\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer.save(\"topic_descriptions\", topic_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df = topic_df[[\"message_id\", \"topic_id\"]].merge(message_df, on=\"message_id\")\n",
    "topics_df = topic_df.merge(topic_descriptions, on=\"topic_id\")[[\"topic_id\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df[\"topic_id\"]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = topic_modellor.get_topic_word_frequencies(topic_df)[[\"topic_id\", \"word\", \"frequency\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer.save(\"topics\", topic_df)\n",
    "checkpointer.save(\"word_frequencies\", word_frequencies)\n",
    "checkpointer.save(\"topic_messages\", message_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Clusters, their Descriptions, and their Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = classify_categories(message_df)\n",
    "checkpointer.save(\"classification\", class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = classify_products(message_df)\n",
    "checkpointer.save(\"products\", product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_df = extract_entities_from_messages(message_df, llm_invoker, use_regex=True)\n",
    "checkpointer.save(\"entities\", entities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summarize_messages(message_df, llm_invoker)\n",
    "checkpointer.save(\"summaries\", summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate list-like columns into new dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_address_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def split_addresses(addresses):\n",
    "        return addresses.split(\",\") if addresses else []\n",
    "\n",
    "    # Explode each address type into separate rows\n",
    "    from_df = pd.DataFrame({\n",
    "        \"message_id\": df[\"message_id\"],\n",
    "        \"address_type\": \"from\",\n",
    "        \"address\": df[\"from_address\"]\n",
    "    })\n",
    "\n",
    "    to_df = df[[\"message_id\", \"to_address\"]].assign(address_type=\"to\")\n",
    "    to_df = to_df.explode(\"to_address\").rename(columns={\"to_address\": \"address\"})\n",
    "\n",
    "    cc_df = df[[\"message_id\", \"cc_address\"]].assign(address_type=\"cc\")\n",
    "    cc_df = cc_df.explode(\"cc_address\").rename(columns={\"cc_address\": \"address\"})\n",
    "\n",
    "    bcc_df = df[[\"message_id\", \"bcc_address\"]].assign(address_type=\"bcc\")\n",
    "    bcc_df = bcc_df.explode(\"bcc_address\").rename(columns={\"bcc_address\": \"address\"})\n",
    "\n",
    "    # Combine all address types into a single dataframe\n",
    "    address_df = pd.concat([from_df, to_df, cc_df, bcc_df], ignore_index=True)\n",
    "\n",
    "    return address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[\"message_id\", \"references\"]].explode(\"references\").rename(columns={\"references\": \"reference_message_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[\"message_id\", \"sender_domain\"]].explode(\"sender_domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df = create_address_df(message_df)\n",
    "reference_df = create_reference_df(message_df)\n",
    "domain_df = create_domain_df(message_df)\n",
    "\n",
    "checkpointer.save(\"addresses\", address_df)\n",
    "checkpointer.save(\"references\", reference_df)\n",
    "checkpointer.save(\"domains\", domain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df.to_csv(config.output_directory + f\"/messages_{DATE}.csv\", index=False)\n",
    "address_df.to_csv(config.output_directory + f\"/addresses_{DATE}.csv\", index=False)\n",
    "reference_df.to_csv(config.output_directory + f\"/references_{DATE}.csv\", index=False)\n",
    "domain_df.to_csv(config.output_directory + f\"/domains_{DATE}.csv\", index=False)\n",
    "word_frequencies.to_csv(config.output_directory + f\"/word_frequencies_{DATE}.csv\", index=False)\n",
    "topics_df.to_csv(config.output_directory + f\"/topics_{DATE}.csv\", index=False)\n",
    "class_df.to_csv(config.output_directory + f\"/classification_{DATE}.csv\", index=False)\n",
    "product_df.to_csv(config.output_directory + f\"/products_{DATE}.csv\", index=False)\n",
    "entities_df.to_csv(config.output_directory + f\"/entities_{DATE}.csv\", index=False)\n",
    "summary_df.to_csv(config.output_directory + f\"/summaries_{DATE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Database\n",
    "# message_df = pd.read_csv('../../data/test/messages.csv')\n",
    "# address_df = pd.read_csv('../../data/test/addresses.csv')\n",
    "# reference_df = pd.read_csv('../../data/test/references.csv')\n",
    "# domain_df = pd.read_csv('../../data/test/domains.csv')\n",
    "# word_frequencies = pd.read_csv('../../data/test/word_frequencies.csv')\n",
    "# topics_df = pd.read_csv('../../data/test/topics.csv')\n",
    "# class_df = pd.read_csv('../../data/test/classifications.csv')\n",
    "# product_df = pd.read_csv('../../data/test/products.csv')\n",
    "# entities_df = pd.read_csv('../../data/test/entities.csv')\n",
    "# summary_df = pd.read_csv('../../data/test/summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 52.53it/s]\n"
     ]
    }
   ],
   "source": [
    "loader.load_dataframe(message_df.replace({np.nan: None}), \"messages\")\n",
    "loader.load_dataframe(address_df.replace({np.nan: None}), \"addresses\")\n",
    "loader.load_dataframe(reference_df.replace({np.nan: None}), \"references\")\n",
    "loader.load_dataframe(domain_df.replace({np.nan: None}), \"domains\")\n",
    "loader.load_dataframe(word_frequencies.replace({np.nan: None}), \"word_frequencies\")\n",
    "loader.load_dataframe(topics_df.replace({np.nan: None}), \"topics\")\n",
    "loader.load_dataframe(class_df.replace({np.nan: None}), \"classifications\")\n",
    "loader.load_dataframe(product_df.replace({np.nan: None}), \"products\")\n",
    "loader.load_dataframe(entities_df.replace({np.nan: None}), \"entities\")\n",
    "loader.load_dataframe(summary_df.replace({np.nan: None}), \"summaries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
